{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 19 exercises"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of lines in the obama_speech.txt file is :  66\n",
      "The total number of words in the obama_speech.txt file is :  2400\n",
      "The total number of words in the obama_speech.txt file is :  2400\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def word_line_count(filename):\n",
    "    lines=[]\n",
    "\n",
    "    # getting number of lines in the file\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines=f.read().splitlines()\n",
    "        print(f\"The total number of lines in the {filename} file is : \", len(lines))\n",
    "\n",
    "    # getting number of words in the file\n",
    "    with open(filename, \"r\") as f:\n",
    "        to_read=f.read()\n",
    "        text=re.sub(r\"^.\", \"\", to_read)\n",
    "        \n",
    "        word_count=text.split()\n",
    "        print(f\"The total number of words in the {filename} file is : \", len(word_count))\n",
    "\n",
    "\n",
    "    # Another way to get the number of the number of words in the file\n",
    "    words=[line[:].split() for line in lines]   \n",
    "    word_count=[y[:] for x in words for y in x]\n",
    "    \n",
    "\n",
    "    print(f\"The total number of words in the {filename} file is : \", len(word_count))\n",
    "    \n",
    "    \n",
    "word_line_count(\"obama_speech.txt\")\n",
    "word_line_count(\"obama_speech.txt\")\n",
    "word_line_count(\"obama_speech.txt\")\n",
    "word_line_count(\"obama_speech.txt\")\n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of languages are 112\n",
      "[('English', 91), ('French', 45), ('Arabic', 25), ('Spanish', 24), ('Portuguese', 9), ('Russian', 9), ('Dutch', 8), ('German', 7), ('Chinese', 5), ('Serbian', 4)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def ten_most_spoken_lan(filename):\n",
    "    with open (filename,\"r\", encoding=\"utf-8\")as f:\n",
    "        json_text=f.read()\n",
    "        data=json.loads(json_text)\n",
    "        \n",
    "        dict_count={} # dictionary to hold the count for each language\n",
    "        for item in data:\n",
    "            for x in item[\"languages\"]:\n",
    "                if x not in dict_count:\n",
    "                    dict_count[x]=1\n",
    "                else:\n",
    "                    dict_count[x]+=1\n",
    "        \n",
    "        # sort in descending order  \n",
    "        dict_count=sorted(dict_count.items(), key=lambda x:x[1], reverse=True)\n",
    "            \n",
    "        # truncate the count for each language\n",
    "        total_lang=[item[:] for item in dict_count ]\n",
    "\n",
    "            \n",
    "        # slice the  top 10 language\n",
    "        top_10_lang=total_lang[:10]\n",
    "\n",
    "        print(f\"The total number of languages are {len(total_lang)}\")\n",
    "        print(top_10_lang)\n",
    "\n",
    "ten_most_spoken_lan(\"countries_data.json\")\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most populous countries are; \n",
      "[('China', 1377422166), ('India', 1295210000), ('United States of America', 323947000), ('Indonesia', 258705000), ('Brazil', 206135893), ('Pakistan', 194125062), ('Nigeria', 186988000), ('Bangladesh', 161006790), ('Russian Federation', 146599183), ('Japan', 126960000)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def most_populated_countries(filename):\n",
    "    with open (filename,\"r\", encoding=\"utf-8\")as f:\n",
    "        json_text=f.read()\n",
    "        data=json.loads(json_text)\n",
    "        \n",
    "        # The 10 most populous countries\n",
    "        # dict to hold each country and its population\n",
    "        dict_count={}\n",
    "\n",
    "        #for loop to iterate through the countries data \n",
    "        # and append the countries alongside its population\n",
    "        # to a dictionary name dict_count\n",
    "\n",
    "        for item in data:\n",
    "            dict_count[item[\"name\"]]=item[\"population\"]\n",
    "        # sort the dictionary in descending order\n",
    "        sort_population=sorted(dict_count.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        # get the 10 most populated \n",
    "        # by slicing the 10 countries to a new list\n",
    "        top_10_most_populated_countries=sort_population[:10]\n",
    "\n",
    "        print(\"The 10 most populous countries are; \")\n",
    "        print(top_10_most_populated_countries)\n",
    "most_populated_countries(\"countries_data.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stephen.marquard@uct.ac.za', 'postmaster@collab.sakaiproject.org', '200801051412.m05ECIaH010327@nakamura.uits.iupui.edu', 'source@collab.sakaiproject.org', 'source@collab.sakaiproject.org', 'source@collab.sakaiproject.org', 'apache@localhost', 'source@collab.sakaiproject.org', 'stephen.marquard@uct.ac.za', 'source@collab.sakaiproject.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern=r\"Received:\\sFrom\\s[A-Za-z0-9.-]+@[A-Za-z-]+\\.edu\"\n",
    "with open (\"email_exchanges_big.txt\", \"r\")as f:\n",
    "    text=f.readlines()\n",
    "    \n",
    "    emails=[]\n",
    "    for line in text:\n",
    "        emails.extend(re.findall(r\"[a-zA-Z0-9]\\S*@\\S*[a-zA-Z]\",line))\n",
    "    print(emails[:10])\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d417a21d97a2e6b8832d21dc44f5cbe4be6e18325f874a8bb8484565b928cd54"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
